{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672a752a-ab81-4bd4-9135-88f8daa322d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pyspark date functions\n",
    "\n",
    "we use this in pipelines for\n",
    "\n",
    "filtering recent data,\n",
    "incremental loads,\n",
    "partitioning data,\n",
    "reporting,\n",
    "time-based metrics\n",
    "CDC pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d24b062-8674-4c38-aa54-6303433654a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    (1, date(2024, 1, 10)),\n",
    "    (2, date(2024, 3, 15)),\n",
    "    (3, date(2025, 1, 5))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data , [\"id\", \"order_date\"])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3129cb-e1d2-4f0d-9bae-cf18303a082d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "current_date()"
    }
   },
   "outputs": [],
   "source": [
    "# use case -> incremental pipelines, audit columns data freshness\n",
    "\n",
    "df.withColumn(\"today\", current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d63a795-7f2a-4dd8-bdd6-cad4d2ec4d1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "datediff()"
    }
   },
   "outputs": [],
   "source": [
    "# datediff() -> difference between dates\n",
    "# use case -> days since order, SLA monitoring, retention metrics\n",
    "\n",
    "df.withColumn(\n",
    "    \"old_days\", datediff(current_date(), col(\"order_date\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecd7244-76fe-439c-bcab-d34edc6c3797",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "year / month / day"
    }
   },
   "outputs": [],
   "source": [
    "#extract year / month / day\n",
    "# use case -> partitioning , reporting, time analysis\n",
    "\n",
    "df.withColumns({\n",
    "    \"year\" : year(col(\"order_date\")),\n",
    "    \"month\" : month(col(\"order_date\")),\n",
    "    \"day\" : dayofmonth(col(\"order_date\"))\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d446ec-841d-4295-bb4f-f80d1db8c3d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add_months()"
    }
   },
   "outputs": [],
   "source": [
    "# add_months () shift date\n",
    "\n",
    "# use case -> subscription expiry, billing \n",
    "\n",
    "df.withColumn(\n",
    "    \"next_month\", add_months(\"order_date\", 1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35a9871-6fac-4127-b96e-1d0586a3cbf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_add() / date_sub()"
    }
   },
   "outputs": [],
   "source": [
    "# date_add() / date_sub() -> add or substract days\n",
    "\n",
    "# use case -> delivery windows, deadline calculations\n",
    "\n",
    "df.withColumn(\"plus_7_days\", date_add(\"order_date\", 7))\\\n",
    "    .withColumn(\"minus_7_days\", date_sub(\"order_date\",7)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd9b587-12a9-4b41-9fa2-d2feaefd16df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "filter() by date range()"
    }
   },
   "outputs": [],
   "source": [
    "# Example : orders after 2024\n",
    "\n",
    "df.filter(col(\"order_date\") > \"2024-01-01\").show()\n",
    "# spark automatically converts string to date, used in incremental loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac27a07-42ee-4597-97e2-48f259328df8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "to_date()"
    }
   },
   "outputs": [],
   "source": [
    "# to_date, -> string -> date conversion\n",
    "# Real often comes like: \"2024-01-10\"\n",
    "# conver string -> date\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "data = [(1, \"2024-01-10\")]\n",
    "\n",
    "df2 = spark.createDataFrame(data, [\"id\", \"date_str\"])\n",
    "df2.withColumn(\n",
    "    \"order_date\",\n",
    "    to_date(col(\"date_str\"), \"yyyy-MM-dd\")\n",
    ").show()\n",
    "\n",
    "# very important in ETL pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbaef509-c9fd-48b6-bd64-cc274cbc698c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_format()"
    }
   },
   "outputs": [],
   "source": [
    "# date_format() -> format date for reporting\n",
    "\n",
    "df.withColumn(\n",
    "    \"formatted_date\",\n",
    "    date_format(\"order_date\", \"yyyy/MM/dd\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbcbb41-4702-4b17-a9e3-07ec0ef60157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Real pipeline Pattern(imp)\n",
    "# string date -> to_date -> filter -> extract year/month -> aggregate\n",
    "# example\n",
    "df.filter(year(\"order_date\") == 2024).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa71d457-b41e-4e64-b384-a21d722b5b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "current_date() -> today\n",
    "\n",
    "datediff() -> days difference\n",
    "\n",
    "year/month/day -> extract parts\n",
    "\n",
    "add_months() -> shift month\n",
    "\n",
    "date_add/sub -> add/substract days\n",
    "\n",
    "to_date() -> string -> date\n",
    "\n",
    "date_format() -> display format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c92bba-a621-4252-bb78-31afe576cf03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add today's date column\n",
    "\n",
    "df2.withColumn(\n",
    "    \"today\" , current_date()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c13f045-7ac8-40ea-91a2-1ab1c6ecb843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calculate days difference\n",
    "\n",
    "df2.withColumns({\n",
    "    \"to_date\" : to_date(col(\"date_str\"), \"yyyy-MM-dd\"),\n",
    "    \"to_timestamp\" : to_timestamp(to_date(col(\"date_str\")), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "    \"to_timestamp2\" : to_timestamp(to_date(col(\"date_str\")), \"yyyy-MM-dd HH:mm:ss.SSS\"),\n",
    "    \"old_days\" : datediff( current_date(), to_date(col(\"date_str\")))\n",
    "                                   \n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f3f020-3162-4b4e-9f22-4535da22c039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract Year + Month\n",
    "\n",
    "df.withColumns({\n",
    "    \"month\" : month(to_date(col(\"order_date\"))),\n",
    "    \"year\" : year(to_date(col(\"order_date\"))),\n",
    "    \"day\" : dayofmonth(to_date(col(\"order_date\")))\n",
    "\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6112c0b-69e5-44fe-acff-760f15f93bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add 30 days to order_date\n",
    "\n",
    "df.withColumn(\n",
    "    \"plus_30_days\", date_add(to_date(col(\"order_date\"), \"yyyy-MM-dd\"), 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd408f8-809f-4584-a3de-de4d63b74c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5ï¸âƒ£ Filter orders from 2024 only\n",
    "\n",
    "df.filter(year(col(\"order_date\")) == 2024).show()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17915aa9-b601-4a82-affa-d468c6748539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF (User defined function)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "data = [\n",
    "    (1, 500),\n",
    "    (2, 150),\n",
    "    (3, 800)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"amount\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fde79af-b862-4bd6-b5b6-0a5eed9f630a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create pyton function\n",
    "# business rule : amount > 300 then high else low\n",
    "\n",
    "def price_category(amount):\n",
    "    if amount >= 300:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"Low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d32660-bcfe-4d0d-9db6-f12dd5e0a333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert it to spark udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "price_udf = udf(price_category, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcb1da4-5f33-475e-9b14-b44c5e2553b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply udf\n",
    "\n",
    "df.withColumn(\n",
    "    \"category\", price_udf(\"amount\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4996f816-9513-49d2-83b2-48ce46a14657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš¨ Problem with UDF\n",
    "\n",
    "When you use UDF:\n",
    "\n",
    "Spark cannot optimize\n",
    "\n",
    "No Catalyst optimization\n",
    "\n",
    "No predicate pushdown\n",
    "\n",
    "Slower execution\n",
    "\n",
    "Python â†” JVM conversion overhead\n",
    "\n",
    "Performance drops.\n",
    "\n",
    "\n",
    "prefer built-in Spark functions over UDF because UDF breaks Spark optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1934d5db-8bf5-4209-bafd-32b6607b3d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# built-in functions\n",
    "from pyspark.sql.functions import col\n",
    "df.withColumn(\n",
    "    \"category\",\n",
    "    when( col(\"amount\") >= 300 ,\"High\").otherwise(\"Low\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3993776-1719-44f6-a42e-e7e7804d503e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pandas udf better than normal\n",
    "\n",
    "# apache Arrow -> vectorized execution\n",
    "# runs faster because process batch of rows\n",
    "\n",
    "# example\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def double_value(x: pd.Series) -> pd.Series:\n",
    "    return x * 2\n",
    "\n",
    "df.withColumn(\"double_amount\", double_value(\"amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a0caf6-b04f-483f-a30a-2950be439c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#task\n",
    "\n",
    "amount >= 500 â†’ \"Premium\"\n",
    "else â†’ \"Regular\"\n",
    "\n",
    "Then implement same logic using when().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd6ab35b-f2dd-4840-b2d6-d27cbe4cc0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Date  & udf functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
