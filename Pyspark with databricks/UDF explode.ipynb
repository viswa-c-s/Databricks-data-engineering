{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f447fa76-4bb8-4222-8dc0-fabd678ec9a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "explode"
    }
   },
   "outputs": [],
   "source": [
    "# Explode converts arrays/lists/ nested values to multiple rows\n",
    "# we use it in > JSON API, kafks, nested events\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data = [\n",
    "    (1, ['Laptop', 'Mouse']),\n",
    "    (2, ['Mobile']),\n",
    "    (3, ['Keyboard', 'Monitor','CPU'])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['order_id', 'items'])\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d79f44-e6d3-4e87-a6d4-a7a939fd78c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_explo = df.select(\n",
    "    'order_id',\n",
    "    explode(\"items\").alias('item')\n",
    ")\n",
    "# for each element in array/list create new row\n",
    "display(df_explo)\n",
    "\n",
    "# imp > explode works only on array/lists map and nested structures, not on strings & numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47825449-f107-4c70-ad8d-d048da98c5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# real example / business problem\n",
    "# each order has multiple items, find total sold items\n",
    "\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "df.select(\n",
    "    explode('items').alias('item'))\\\n",
    "        .groupBy('item').count().show()\n",
    "        \n",
    "#explode is used to flatten nested array or map columns into multiple rows for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a830257-9407-4f92-94c2-b19332aae85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataa = [\n",
    "    (1, ['Phone', 'Charger']),\n",
    "    (2, ['Laptop'])\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(dataa, ['Customer_id', 'Products'])\n",
    "df1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2fcada-e3d0-426d-a000-2c079b820c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, explode\n",
    "\n",
    "df1.select(\n",
    "    explode(\"Products\").alias(\"Product\")\n",
    ").groupBy(\"Product\").agg(count(\"*\").alias(\"total\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8dfd1c9-c4ac-4430-9f35-4ba6f7c0cced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, explode\n",
    "\n",
    "df1.select(\n",
    "    explode(\"Products\").alias(\"Product\")\n",
    ").groupBy(\"Product\") \\\n",
    " .agg(count(\"*\").alias(\"total\")) \\\n",
    " .show(truncate=False)\n",
    "\"\"\"\n",
    "count(\"*\") is a Column expression\n",
    "\n",
    "groupBy().agg() expects Column expressions\n",
    "\n",
    "groupBy().count() is a shortcut that doesn't accept arguments\n",
    "\n",
    "| Column Type   | What To Do                 |\n",
    "| ------------- | -------------------------- |\n",
    "| Array         | `explode` → then `groupBy` |\n",
    "| String/Scalar | Just `groupBy`             |\n",
    "| Map           | `explode` → then `groupBy` |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeca4038-18fb-4bf6-835e-949e5945a598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| Function      | Purpose               |\n",
    "| ------------- | --------------------- |\n",
    "| explode       | flatten array         |\n",
    "| explode_outer | keeps null rows       |\n",
    "| posexplode    | returns index + value |\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6de6746-d9ba-48be-9946-9f172bf1d4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, count\n",
    "\n",
    "df_exploded = df1.select(\n",
    "    explode(\"Products\").alias(\"Product\")\n",
    ")\n",
    "\n",
    "df_sales = df_exploded.groupBy(\"Product\").agg(\n",
    "    count(\"Product\").alias(\"total\")\n",
    ")\n",
    "\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eaa3ff3-ee3a-498d-bc88-0676746d9ef2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String functions"
    }
   },
   "outputs": [],
   "source": [
    "# String functions are used in data cleaning , parsing text, fixing formats, transforming raw filed\n",
    "\n",
    "from pyspark.sql.functions import length, trim, ltrim, rtrim, lower, upper, initcap, lpad, rpad, substring, regexp_replace\n",
    "\n",
    "data = [\n",
    "    (1, \"   Laptop,Mouse   \", \"Arjun\", \"Kumar\"),\n",
    "    (2, \"Mobile,Charger\", \"Ravi\", \"raj\"),\n",
    "    (3, \"Tablet\", \"Meena\", \"S\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"items_str\", \"first\", \"last\"])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adfd9d8-956e-4054-983a-103cdee1c569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.withColumn(\n",
    "    \"Cleaned_items\", trim(col(\"items_str\"))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9b691b-a0d2-4561-b849-3bf631e8af33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, lit, concat\n",
    "df.withColumns({\n",
    "    \"Cleaned_items\": trim(col(\"items_str\")), \n",
    "    \"items_array\": split(trim(col(\"items_str\")), \",\"),\n",
    "    \"Full_Name\": concat(col(\"first\"), lit(\" \"), col(\"last\"))}\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf27ea56-a24f-4014-ba5a-8f5e60b8a280",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"items_array\":149},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771168386405}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.withColumns({\n",
    "    \"Cleaned_items\": trim(col(\"items_str\")), \n",
    "    \"items_array\": split(trim(col(\"items_str\")), \",\"),\n",
    "    \"Full_Name\": concat(col(\"first\"), lit(\" \"), col(\"last\"))}\n",
    "))#.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abea821-a310-46c0-ba74-8f24fadd7746",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "concat_ws"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "# cleaner concating for many columns join with separator\n",
    "df.withColumn(\n",
    "    \"Full_Name\", concat_ws(\" \", col(\"first\"), col(\"last\"))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4b17f2-8d48-422e-825e-934cf458b25d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "lower & upper"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.withColumn(\"First_Lower\", lower(col(\"first\")))\\\n",
    "    .withColumn(\"Last_Upper\", upper(col(\"last\"))).show(truncate=False)\n",
    "\n",
    "# used in before joins & comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d544778-c5d5-406e-b7d8-2dc80cdcb099",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "substring"
    }
   },
   "outputs": [],
   "source": [
    "# substring - extract part of string\n",
    "from pyspark.sql.functions import substring, col\n",
    "\n",
    "\n",
    "df.withColumn(\n",
    "    \"Short_name\",\n",
    "    substring(col(\"first\"), 1, 3)\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a435bd8-0857-49d0-ab9d-345deb6231b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "regexp_replace()"
    }
   },
   "outputs": [],
   "source": [
    "# regexp_replace - remove unwanted characters\n",
    "# used for removing special characters, phone number cleaning, log parsing, messy data cleanup\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df.withColumn(\n",
    "    \"Clean_text\",\n",
    "    regexp_replace(col(\"items_str\"), \",\", \"\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4beb341-3bf4-4540-afcf-0ffe0a93424f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# trim -> split -> explode -> aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfd53d0-6f3e-4a38-9cee-98a6a92729da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, count, split\n",
    "\n",
    "df.select(\n",
    "    explode(split(trim(col(\"items_str\")), \",\")).alias(\"item\")\n",
    ").groupBy(\"item\").agg(count(\"*\").alias(\"total\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13aa47bc-a3e6-4b62-bc7c-15f0c49fe0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "trim()            → remove spaces,\n",
    "split()           → string → array,\n",
    "concat_ws()       → combine columns,\n",
    "lower/upper()     → standardize text,\n",
    "substring()       → extract part,\n",
    "regexp_replace()  → clean patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "504ae331-acd8-417a-9b24-1908987a5574",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "tasks"
    }
   },
   "outputs": [],
   "source": [
    "# remove spaces from items_str\n",
    "\n",
    "df.withColumn(\n",
    "    \"cleaned_items\", trim(col(\"items_str\"))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba715df-04a1-4ca5-9641-4f2ec3f895a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into array\n",
    "\n",
    "df.withColumn(\n",
    "    \"splited_items\", split(trim(col(\"items_str\")), \",\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0f0a76-4a05-49f9-8521-bb9dade93ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#explode items\n",
    "from pyspark.sql.functions import explode\n",
    "df.withColumn(\n",
    "    \"exploded_items\", explode(split(trim(col(\"items_str\")), \",\"))\n",
    ").show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64dc5e03-81b9-4faf-ac0a-0b985447b1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count each item\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.withColumn(\n",
    "    \"exploded_items\", explode(split(trim(col(\"items_str\")), \",\"))\n",
    ").groupBy(\"exploded_items\").agg(count(\"*\").alias(\"total\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250c1e7a-a741-4a98-bd4b-d365ffea7b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create full name in lowercase\n",
    "from pyspark.sql.functions import concat_ws\n",
    "df.withColumn(\n",
    "    \"Full_name\" , concat_ws(\" \", lower(col(\"first\")), lower(col(\"last\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56291b6f-237f-48dc-a237-82153fb0f8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\n",
    "    explode(split(trim(col(\"items_str\")), \",\")).alias(\"item\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UDF explode",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
