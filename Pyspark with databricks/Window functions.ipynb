{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d323cb5-9904-4a03-a599-80a033dbf6f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use Window functions to do row level analytics without collapsing(unlike groupBy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c83fde7-e2c0-4026-9735-d5f84a79d4eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Why Window functions ?\n",
    "# groupBy -> reduces rows\n",
    "# Window functions -> does not reduce rows -> keeps all rows + adds insight\n",
    "# Window functions\n",
    "# - rank, dense_rank, row_number\n",
    "# - lead, lag\n",
    "# - first_value, last_value\n",
    "# - sum, avg, min, max\n",
    "# - count\n",
    "\n",
    "# you will use window when : you need a rank, lastest records, deduplication, moving average, runnin total etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001e2100-6eb3-4201-bf23-a5df62f8080e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "\n",
    "| Function       | What it does           |\n",
    "| -------------- | ---------------------- |\n",
    "| `row_number()` | Unique row number      |\n",
    "| `rank()`       | Ranking with gaps      |\n",
    "| `dense_rank()` | Ranking without gaps   |\n",
    "| `lag(col, n)`  | Previous row value     |\n",
    "| `lead(col, n)` | Next row value         |\n",
    "| `sum(col)`     | Running / windowed sum |\n",
    "| `avg(col)`     | Window average         |\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4df800f-e346-43a2-b425-7405a2524080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a window spec (Window Specification) defines how rows are grouped and ordered when you apply window functions—things like row_number, rank, lag, lead, running totals, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa83a4c6-0287-4b31-9eba-5d284383150a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A window spec can have three main parts:\n",
    "\n",
    "1. partitionBy – how to group rows (like GROUP BY, but without collapsing rows)\n",
    "\n",
    "2. orderBy – how rows are ordered within each partition\n",
    "\n",
    "3. rowsBetween / rangeBetween – frame boundaries (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8921d79-063c-4614-b890-2ec27846019e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read data"
    }
   },
   "outputs": [],
   "source": [
    "df_orders = (spark.read.table(\"dev_data.test.orders_online\"))\n",
    "\n",
    "df_customers = (spark.read.table(\"dev_data.test.customers_online\"))\n",
    "df_orders.show()\n",
    "#df_customers.show()\n",
    "\n",
    "# try to avoid double actions at same cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84f1bb0-bb8e-42fe-8b6f-260017908e9e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "window spec"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1ea809-a23c-424a-8419-e2c3451cb315",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "row_number"
    }
   },
   "outputs": [],
   "source": [
    "# row_numer most imp\n",
    "# lastest order per costumer\n",
    "from pyspark.sql.functions import col\n",
    "df_lastest = (\n",
    "    df_orders.withColumn(\n",
    "        \"rn\", row_number().over(window)\n",
    "    )\n",
    ")\n",
    "df_lastest.filter(col(\"rn\") == 1).show()\n",
    "\n",
    "# row_number is used to identify the lastest or top record per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7d116e-6268-48d7-a4e0-64c33b02536f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lastest.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f1f7d2-37ea-4339-9c52-7848735de5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lastest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54874562-3439-4b42-978b-fb821ad4db42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "#d = rank().over(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d17c37-17a6-455f-9e89-930cbb5eacbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dense_rank().over(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896abbc4-6945-444a-9cf5-cdcb7a14d7aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "deduplication pattern"
    }
   },
   "outputs": [],
   "source": [
    "# keep the latest record per customer\n",
    "\n",
    "df_dedup = (\n",
    "    df_orders.withColumn(\n",
    "        \"rn\", row_number().over(window)\n",
    "    ).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    ")\n",
    "\n",
    "df_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05a8e64-8e46-4254-b3d8-e735daa279f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# running total\n",
    "from pyspark.sql.functions import sum, col\n",
    "runnin_window = (\n",
    "    Window.partitionBy(\"customer_id\")\\\n",
    "        .orderBy(\"order_date\")\\\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "df_running = (\n",
    "    df_orders.withColumn(\n",
    "        \"running_total\", \n",
    "        sum(col(\"order_amount\").cast(\"int\")).over(runnin_window)\n",
    "    )\n",
    ")\n",
    "\n",
    "df_running.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebe9a53-1c86-453c-bc8e-492c9ed2f71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#latest Order per customer\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "win_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"order_date\").desc())\n",
    "\n",
    "lastest_ord = df_orders.withColumn(\n",
    "    \"rn\", row_number().over(win_spec))\n",
    "\n",
    "display(lastest_ord.filter(col(\"rn\") ==1))\n",
    "lastest_ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04309e3d-70e1-45f4-988b-b35de5bd9a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lastest_ord = df_orders.withColumn(\n",
    "    \"rn\", row_number().over(win_spec))\\\n",
    "        .filter(col(\"rn\") ==1).drop(\"rn\")\n",
    "\n",
    "display(lastest_ord)\n",
    "lastest_ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c47bf51-809b-4585-83e9-eddbd6ab7252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2 rank orders by amount per customer\n",
    "\n",
    "from pyspark.sql.functions import rank, sum\n",
    "\n",
    "win_rank = Window.partitionBy(\"customer_id\").orderBy(col(\"order_amount\").desc())\n",
    "\n",
    "df_rank = df_orders.withColumn(\n",
    "    \"rank\", rank().over(win_rank))\n",
    "display(df_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f5beda-c6be-4a25-9f99-19d54ecac633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3️⃣ Deduplicate orders keeping latest\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"order_date\").desc())\n",
    "\n",
    "df_dedup1 = df_orders.withColumn(\n",
    "    \"rn\", row_number().over(window_spec)\n",
    ").filter(col(\"rn\") ==1).drop(\"rn\")\n",
    "\n",
    "display(df_dedup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44073d9-a078-4fa9-a349-f54626dc58f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4️⃣ Running total per customer\n",
    "\n",
    "wind_run = (Window.partitionBy(\"customer_id\")\n",
    "            .orderBy(col(\"order_date\"))\n",
    "            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "running_total_df = df_orders.withColumn(\n",
    "    \"Running_total\", sum(col(\"order_amount\")).over(wind_run)\n",
    ")\n",
    "\n",
    "display(running_total_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db986df-1ad0-4623-9fc8-6b0a3b037fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Window functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
