{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc956de-d52c-4b50-a885-db002056dea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5b562b-91de-4b7f-82f8-0d6b35220c9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark Session"
    }
   },
   "outputs": [],
   "source": [
    "# spark session creation manually\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"SparkFunctions\").getOrCreate()\n",
    "\n",
    "# SparkSession is the unified entry point to spark /functionality / sql and dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c89d4a6-074c-43f0-baeb-8181281d96cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DataFrame is immutable, every operation creates a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1ce7f6-8648-404d-b313-b4990ab9c431",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataFrame creattion"
    }
   },
   "outputs": [],
   "source": [
    "# create a simple dataframe\n",
    "\n",
    "data = [\n",
    "  (1, \"Arjun\", \"Chennai\", 50000),\n",
    "  (2, \"Ravi\", \"Bangalore\", 60000),\n",
    "  (3, \"Meena\", None, 70000),\n",
    "  (4, \"Raj\", \"Delhi\", 80000)\n",
    "]\n",
    "\n",
    "column = [\"Id\", \"Name\", \"City\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb307ff-42e5-454a-8e95-2fa4a3e9c14f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "show() & truncate"
    }
   },
   "outputs": [],
   "source": [
    "# truncate = False used show full columns text/value\n",
    "# show() is an action - it triggers the computation/execution\n",
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319fae96-977f-4072-9de8-344b91f8129f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "printSchema()"
    }
   },
   "outputs": [],
   "source": [
    "#printSchema\n",
    "# printSchema helps to validate inferred schema before transfermations.\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26646829-a459-4a6a-8d72-24e91be13c07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select()"
    }
   },
   "outputs": [],
   "source": [
    "# select() -> is like choosing columns from a table(sql select)\n",
    "\n",
    "df.select(\"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e330f6b0-318e-4712-8fd1-9889eeacba57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select with expression\n",
    "\n",
    "df.select(df.Name, df.Salary + 1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b113a61d-d7ea-49ce-b2ca-512a81eef741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Alias & col"
    }
   },
   "outputs": [],
   "source": [
    "# Alias & col\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\").alias(\"monthly_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0935c061-cb03-4119-9123-e3d691ef9671",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "withColumn()"
    }
   },
   "outputs": [],
   "source": [
    "# withColumn() -> add / modify columns\n",
    "# always returns a new dataframe\n",
    "\n",
    "df2 = df.withColumn(\"annual_salary\", col(\"salary\") * 12)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e420e36f-8ac8-430c-9394-8ad0caff9497",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "cast()"
    }
   },
   "outputs": [],
   "source": [
    "# cast() -> fixing datatypes\n",
    "# we use it in Joins, Aggregations and other transformations writing to delta / warehouse\n",
    "\n",
    "df_cast = df.withColumn(\n",
    "    \"salary\",\n",
    "    col(\"salary\").cast(\"int\")\n",
    ")\n",
    "\n",
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba39eaf-982f-4a3b-83fd-98118307302d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename dataframe(Joins later)\n",
    "# used in when joining same table & avoiding column ambiguity\n",
    "df.alias(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a720d5a1-fd77-48b3-b9fa-c5ea3032fe92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lazy Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "# this code does not execute immediately, it is a lazy operation\n",
    "# it is executed when an action is called\n",
    "# it is a transformation\n",
    "df.select(\"name\")\n",
    "\n",
    "# execution happens only when : show(), count(), take(), collect(), write()\n",
    "\n",
    "# Spark builds a logical plan and executes only when an action is called\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e175fce7-81b1-413c-a736-987620d3f7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe for laptop\n",
    "\n",
    "device_data = [\n",
    "    (101,\"Laptop\" ,\"Dell\", 50000),\n",
    "    (102,\"Laptop\", \"HP\", 45000),\n",
    "    (103,\"Laptop\", \"Lenovo\", 40000),\n",
    "    (104, \"SmartPhone\", \"Redmi\", 15000)\n",
    "]\n",
    "\n",
    "device_column = [\"Id\", \"Product\", \"Brand\", \"Price\"]\n",
    "\n",
    "df_device = spark.createDataFrame(device_data, device_column)\n",
    "df_device.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8beb3a-359f-46db-bac3-74b0f577c799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_offer = df_device.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
    "df_offer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658b6141-ac06-43d4-a4f1-530e0bb4d4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_offer.select(df_offer.Product, df_offer.discounted_price).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebcaa78c-9a70-43d4-9992-4e5793264d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_core_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
